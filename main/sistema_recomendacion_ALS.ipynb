{
  "metadata": {
    "name": "Sistema de Recomendación",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d KEYWORDS \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.sql import SparkSession\nimport json\nfrom pyspark.sql.functions import from_json, col, explode,pandas_udf ,from_unixtime,regexp_extract, explode, split, udf, window\nfrom pyspark.sql.types import *\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport ast\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d DEFINICION DE VARIABLES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nverdadero \u003d\u0027true\u0027\ninput_bucket \u003d \u0027s3://data-emr-motor/movies_parquet\u0027\ninput_path_keywords \u003d \u0027/keywords/\u0027\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d LECTURA DE ARCHIVO DESDE S3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\ndf_keywords\u003d spark.read.option(\"header\",verdadero).parquet(input_bucket + input_path_keywords)\nSchema \u003d StructType([StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True)])\ndf_select \u003d df_keywords.withColumn(\u0027keywords\u0027,regexp_replace(col(\u0027keywords\u0027),\u0027[\\\\[\\\\]]\u0027,\u0027\u0027))\ndf_select \u003d df_select.withColumn(\"keywords\", regexp_replace(col(\u0027keywords\u0027), \u0027},\u0027, \u0027};\u0027))\ndf_select \u003d df_select.select(explode(split(col(\"keywords\"), \u0027;\u0027)).alias(\"separated\"), col(\"id\"))\ndf_select \u003d df_select.withColumnRenamed(\"id\", \"id_o\")\ndf_select \u003d df_select.select(from_json(col(\"separated\"), Schema).alias(\"separados\"), \"id_o\")\\\n            .select(\"separados.id\", \"separados.name\", \"id_o\")\ndf_select.filter(\u0027id is not null\u0027).show(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n \r\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d RAITINGS SMALL \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\r\n\r\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d DEFINICION DE VARIABLES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\r\n\r\nverdadero \u003d\u0027true\u0027\r\n#input_bucket \u003d \u0027s3://data-emr-motor\u0027\r\ninput_path_ratings_small \u003d \u0027/ratings_small/\u0027\r\n\r\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d LECTURA DE ARCHIVO DESDE S3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\r\n\r\ndf_rating_small\u003d spark.read.option(\"header\",verdadero).parquet(input_bucket + input_path_ratings_small)\r\ndf_rating_small.show(5)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d RAITINGS FULL \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d DEFINICION DE VARIABLES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nverdadero \u003d\u0027true\u0027\ninput_path_ratings \u003d \u0027/ratings/\u0027\n\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d LECTURA DE ARCHIVO DESDE S3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\ndf_ratings\u003d spark.read.option(\"header\",verdadero).parquet(input_bucket + input_path_ratings)\ndf_ratings.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d MOVIES META-DATOS \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d DEFINICION DE VARIABLES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nverdadero \u003d\u0027true\u0027\ninput_path_metadata \u003d \u0027/movies_metadata/\u0027\n\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d LECTURA DE ARCHIVO DESDE S3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\ndf_movies_metadata\u003d spark.read.option(\"header\",verdadero).parquet(input_bucket + input_path_metadata)\ndf_movies_metadata \u003d df_movies_metadata.withColumn(\"vote_average\", df_movies_metadata[\"vote_average\"].cast(IntegerType()))\ndf_movies_metadata \u003d df_movies_metadata.withColumn(\"vote_count\", df_movies_metadata[\"vote_count\"].cast(IntegerType()))\nCOLS_movies \u003d [\u0027id\u0027,\u0027title\u0027,\u0027vote_average\u0027,\u0027vote_count\u0027]\ndf_movies_metadata \u003d df_movies_metadata.select(*COLS_movies)\ndf_movies_metadata.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql.functions import desc\n\nt1 \u003d df_ratings.alias(\u0027t1\u0027)\nt2 \u003d df_movies_metadata.alias(\u0027t1\u0027)\n\nbase_recomendacion\u003d t1.join(t2, t1.movieId \u003d\u003d t2.id, \u0027leftouter\u0027)\nbase_recomendacion \u003dbase_recomendacion.filter(\u0027vote_count is not null\u0027)\n\nbase_recomendacion.filter(\u0027id is not null\u0027).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmovie \u003d base_recomendacion.groupBy(\"movieId\").count().sort(col(\"count\").desc())\nmovie.count()\nmovie \u003d movie.filter(col(\u0027count\u0027)\u003c\u003d100)\n\nuser \u003d base_recomendacion.groupBy(\"userId\").count().sort(col(\"count\").desc())\nuser.count()\nuser \u003d user.filter(col(\u0027count\u0027)\u003c\u003d200)\n\n\nbase_recomendacion \u003d base_recomendacion.join(movie,\u0027movieId\u0027,\u0027leftanti\u0027)\nbase_recomendacion \u003d base_recomendacion.join(user,\u0027userId\u0027,\u0027leftanti\u0027)\nbase_recomendacion.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nbase_recomendacion\u003dbase_recomendacion.filter(base_recomendacion.title.isNotNull())\nbase_recomendacion\u003dbase_recomendacion.filter(base_recomendacion.vote_average.isNotNull())\nbase_recomendacion\u003dbase_recomendacion.filter(base_recomendacion.vote_count.isNotNull())\nbase_recomendacion\u003dbase_recomendacion.filter(base_recomendacion.id.isNotNull())\nbase_recomendacion \u003d base_recomendacion.withColumn(\u0027vote_count\u0027, regexp_replace(\u0027vote_count\u0027, \u0027/[^a-z0-9-]\u0027, \u0027ln\u0027))\nbase_recomendacion \u003d base_recomendacion.withColumn(\u0027vote_average\u0027, regexp_replace(\u0027vote_average\u0027, \u0027/[^a-z0-9-]\u0027, \u0027ln\u0027))\nbase_recomendacion \u003d base_recomendacion.withColumn(\"vote_count\", col(\"vote_count\").astype(IntegerType()))\nbase_recomendacion\u003dbase_recomendacion.filter(base_recomendacion.vote_count \u003c\u003d 2000)\n\n\nbase_recomendacion.describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nmin_rating \u003d 0.5\nmax_rating \u003d 5\nbase_recomendacion \u003d base_recomendacion.withColumn(\u0027norm_rating\u0027, (base_recomendacion.rating-min_rating)/(max_rating-min_rating))\nbase_recomendacion.describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Las 10 peliculas más calificadas\ntop_most_rated_movies \u003d base_recomendacion.groupBy(\"title\").sum(\"vote_count\").sort(col(\"sum(vote_count)\").desc())\ntop_most_rated_movies \u003d top_most_rated_movies.withColumnRenamed(\"sum(vote_count)\", \"count\")\ntop_most_rated_movies.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Las 10 peliculas mejor calificadas\navg_rating_by_movie \u003d base_recomendacion.groupBy(\"title\").avg(\"norm_rating\").sort(col(\"avg(norm_rating)\").desc())\navg_rating_by_movie \u003d avg_rating_by_movie.withColumnRenamed(\"avg(norm_rating)\", \"avg_rating\")\navg_rating_by_movie \u003d avg_rating_by_movie.withColumnRenamed(\"title\", \"movie_title\")\navg_rating_by_movie.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Relación la calificación y la popularidad\ntop_movies \u003d top_most_rated_movies.join(avg_rating_by_movie, top_most_rated_movies[\"title\"] \u003d\u003d avg_rating_by_movie[\"movie_title\"], \"left_outer\").drop(\u0027movie_title\u0027).withColumn(\"ratio\",col(\"avg_rating\")/col(\"count\"))\ntop_movies.select(\"title\", \"ratio\").sort(col(\"ratio\").desc()).show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#Top 10 Palabras clave\nkeywords \u003d df_select.groupBy(\"name\").count().sort(col(\"count\").desc())\nkeywords \u003dkeywords.withColumnRenamed(\"name\", \"keyword\")\nkeywords.filter(\u0027keyword is not null\u0027).show(10) "
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#import pandas as pd\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder.appName(\u0027recnn\u0027).getOrCreate()\n\nbase_recomendacion.describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.sql import types \n\nbase_recomendacion\u003dbase_recomendacion.withColumn(\"userId\", base_recomendacion[\"userId\"].cast(IntegerType()))\nbase_recomendacion\u003dbase_recomendacion.withColumn(\"movieId\", base_recomendacion[\"movieId\"].cast(IntegerType()))\nbase_recomendacion \u003d base_recomendacion.withColumnRenamed(\"norm_rating\", \"label\")\nbase_recomendacion.show(5)\nprint(base_recomendacion)\n\n(training, test) \u003d base_recomendacion.randomSplit([0.8, 0.2])"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\n\n# Build the recommendation model using ALS on the training data\nals \u003d ALS(userCol\u003d\"userId\", itemCol\u003d\"movieId\", ratingCol\u003d\"label\", coldStartStrategy\u003d\"drop\", seed\u003d0, nonnegative\u003dTrue)\n# Set considered parameter grid\nparamGrid \u003d ParamGridBuilder().addGrid(als.regParam, [0.1, 0.05, 0.01]).addGrid(als.rank, [4, 8, 12]).build()\n\n# Set evaluator\nmodelEvaluator \u003d RegressionEvaluator(metricName\u003d\"rmse\")\n\n# Set cross validator instance\ncrossval \u003d CrossValidator(estimator\u003dals,\n                          estimatorParamMaps\u003dparamGrid,\n                          evaluator\u003dmodelEvaluator,\n                          numFolds\u003d5)\n\n# Perform cross-validation\ncvModel \u003d crossval.fit(training)\n#model \u003d als.fit(training)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%spark.pyspark\n# Selección de modelo\nbest_als_model \u003d cvModel.bestModel\nprint(\"Best number of latent factors (rank parameter): \" + str(best_als_model.rank))\nprint(\"Best value of regularization factor: \" + str(best_als_model._java_obj.parent().getRegParam()))\nprint(\"Max Iterations: \" + str(best_als_model._java_obj.parent().getMaxIter()))"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Make predictions on a random test subset obtained through randomSplit()\nprint(\"Predictions based on a random test subset:\")\npredictions \u003d best_als_model.transform(test)\npredictions.show(5)\n\n# Evaluate model\u0027s performance on test (evaluate overfitting)\ndef overfitting_evaluation(predictions):\n    # Model evaluation in test - ratings regression evaluation\n    print(\"Model evaluation on test data:\")\n    predictions \u003d predictions.na.drop()\n    # RMSE\n    rmse_evaluator \u003d RegressionEvaluator(metricName\u003d\"rmse\", labelCol\u003d\"label\", predictionCol\u003d\"prediction\")\n    rmse \u003d rmse_evaluator.evaluate(predictions)\n    print(\"Root-mean-square error (RMSE) \u003d \" + str(rmse))\n    # MSE\n    mse_evaluator \u003d RegressionEvaluator(metricName\u003d\"mse\", labelCol\u003d\"label\", predictionCol\u003d\"prediction\")\n    mse \u003d mse_evaluator.evaluate(predictions)\n    print(\"Mean-square error (MSE) \u003d \" + str(mse))\n    # R2\n    r2_evaluator \u003d RegressionEvaluator(metricName\u003d\"r2\", labelCol\u003d\"label\", predictionCol\u003d\"prediction\")\n    r2 \u003d r2_evaluator.evaluate(predictions)\n    print(\"r² metric \u003d \" + str(r2))\n    # MAE\n    mae_evaluator \u003d RegressionEvaluator(metricName\u003d\"mae\", labelCol\u003d\"label\", predictionCol\u003d\"prediction\")\n    mae \u003d mae_evaluator.evaluate(predictions)\n    print(\"Mean Absolute Error (MAE) \u003d \" + str(mae))\n\n    return [rmse, mse, r2, mae]\n\nrandom_test_eval \u003d overfitting_evaluation(predictions)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ndef kfold_test_eval(df, Kfolds\u003d5):\n    rmse_evaluations \u003d []\n    mse_evaluations \u003d []\n    r2_evaluations \u003d []\n    mae_evaluations \u003d []\n    \n    for k in range(0, Kfolds):  \n        (train, test) \u003d df.randomSplit([0.8, 0.2])\n        tunned_als \u003d als \u003d ALS(userCol\u003d\"userId\", itemCol\u003d\"movieId\", ratingCol\u003d\"label\", coldStartStrategy\u003d\"drop\", maxIter\u003d10, regParam\u003d0.01, rank\u003d12)\n        model \u003d tunned_als.fit(train)\n        predictions \u003d model.transform(test)\n        print(\"Kfold: \" + str(k + 1))\n        k_test_eval \u003d overfitting_evaluation(predictions)\n        rmse_evaluations.append(k_test_eval[0])\n        mse_evaluations.append(k_test_eval[1])\n        r2_evaluations.append(k_test_eval[2])\n        mae_evaluations.append(k_test_eval[3])\n        \n    average_rmse \u003d sum(rmse_evaluations)/float(len(rmse_evaluations))\n    average_mse \u003d sum(mse_evaluations)/float(len(mse_evaluations))\n    average_r2 \u003d sum(r2_evaluations)/float(len(r2_evaluations))\n    average_mae \u003d sum(mae_evaluations)/float(len(mae_evaluations))\n    \n    return [average_rmse, average_mse, average_r2, average_mae]\n\n[average_rmse, average_mse, average_r2, average_mae] \u003d kfold_test_eval(training)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nprint(\"Average Root-mean-square error (RMSE) \u003d \" + str(average_rmse))\nprint(\"Average Mean-square error (MSE) \u003d \" + str(average_mse))\nprint(\"Average r² metric \u003d \" + str(average_r2))\nprint(\"Average Mean Absolute Error (MAE) \" + str(average_mae))"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Generate top 10 profiles recommendations for each user\nuserRecs \u003d best_als_model.recommendForAllUsers(5)\nuserRecs.show(5, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nbase_recomendacion.filter(base_recomendacion.userId \u003d\u003d 24).show()\n#df.filter(df.state \u003d\u003d \"OH\").show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Generate top 10 user recommendations for each profile\nprofileRecs \u003d best_als_model.recommendForAllItems(10)\nprofileRecs.show(1, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# Generate top 10 profile recommendations for a set of 10 users\nusers \u003d test.select(als.getUserCol()).distinct().limit(10)\nuserSubsetRecs \u003d best_als_model.recommendForUserSubset(users, 10)\nuserSubsetRecs.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprofiles \u003d test.select(als.getItemCol()).distinct().limit(10)\nprofileSubSetRecs \u003d best_als_model.recommendForItemSubset(profiles, 10)\nprofileSubSetRecs.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\npredictions \u003d best_als_model.transform(test)\npredictions.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nt2.registerTempTable(\"processed_movies_metadata\")"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf \u003d sqlContext.read.parquet(\u0027/movies_metadata/\u0027)\ndf.registerTempTable(\"processed_movies_metadata\")\ndf2 \u003d sqlContext.read.parquet(\u0027/ratings_small/\u0027)\ndf2.registerTempTable(\"processed_ratings_small\")\ndf2 \u003d sqlContext.read.parquet(\u0027/process/ratings\u0027)\ndf2.registerTempTable(\"processed_ratings\")\ndf2 \u003d sqlContext.read.parquet(\u0027/home/david/PycharmProjects/emr-demo/process/keywords\u0027)\ndf2.registerTempTable(\"processed_keywords\")\ndf2 \u003d sqlContext.read.parquet(\u0027/home/david/PycharmProjects/emr-demo/process/credits\u0027)\ndf2.registerTempTable(\"processed_credits\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d KEYWORDS \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import regexp_replace\nfrom pyspark.sql import SparkSession\nimport json\nfrom pyspark.sql.functions import from_json, col, explode,pandas_udf ,from_unixtime,regexp_extract, explode, split, udf, window\nfrom pyspark.sql.types import *\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport ast\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d DEFINICION DE VARIABLES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nverdadero \u003d\u0027true\u0027\ninput_bucket \u003d \u0027s3://data-emr-motor/movies_parquet\u0027\ninput_path_keywords \u003d \u0027/credits/\u0027\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d LECTURA DE ARCHIVO DESDE S3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\ndf_credits1\u003d spark.read.option(\"header\",verdadero).parquet(input_bucket + input_path_keywords)\nSchema2 \u003d StructType([StructField(\"cast_id\", IntegerType(), True), StructField(\"character\", StringType(), True)])\ndf_credits \u003d df_credits1.withColumn(\u0027cast\u0027,regexp_replace(col(\u0027cast\u0027),\u0027[\\\\[\\\\]]\u0027,\u0027\u0027))\ndf_credits \u003d df_credits.withColumn(\"cast\", regexp_replace(col(\u0027cast\u0027), \u0027},\u0027, \u0027};\u0027))\n\ndf_credits \u003d df_credits.select(explode(split(col(\"cast\"), \u0027;\u0027)).alias(\"separated\"), col(\"id\"))\ndf_credits \u003d df_credits.withColumnRenamed(\"id\", \"id_o\")\ndf_credits \u003d df_credits.select(from_json(col(\"separated\"), Schema2).alias(\"separados\"), \"id_o\")\\\n            .select(\"id_o\", \"separados.cast_id\", \"separados.character\", )\n\ndf_credits.show()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndf_select \u003d df_keywords.withColumn(\u0027keywords\u0027,regexp_replace(col(\u0027keywords\u0027),\u0027[\\\\[\\\\]]\u0027,\u0027\u0027))\ndf_select \u003d df_select.withColumn(\"keywords\", regexp_replace(col(\u0027keywords\u0027), \u0027},\u0027, \u0027};\u0027))\ndf_select \u003d df_select.select(explode(split(col(\"keywords\"), \u0027;\u0027)).alias(\"separated\"), col(\"id\"))\ndf_select \u003d df_select.withColumnRenamed(\"id\", \"id_o\")\ndf_select \u003d df_select.select(from_json(col(\"separated\"), Schema).alias(\"separados\"), \"id_o\")\\\n            .select(\"separados.id\", \"separados.name\", \"id_o\")\ndf_select.filter(\u0027id is not null\u0027).show(5)"
    }
  ]
}