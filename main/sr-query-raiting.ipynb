{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "name": "motor",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "colab": {
      "name": "motor.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "auto",
        "id": "y-GoEwGBp3dU"
      },
      "source": [
        "%spark.pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_replace, col, expr, countDistinct\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import IntegerType\n",
        "verdadero='true'\n",
        "\n",
        "input_bucket = 's3://data-emr-motor/movies_parquet/'\n",
        "\n",
        "# to read parquet file\n",
        "df_keywords= spark.read.option(\"header\",verdadero).parquet(input_bucket + 'movies_metadata/')\n",
        "df_keywords.createOrReplaceGlobalTempView(\"processed_movies_metadata\")\n",
        "df_keywords2= spark.read.option(\"header\",verdadero).parquet(input_bucket + 'ratings_small/')\n",
        "df_keywords2.createOrReplaceGlobalTempView(\"processed_ratings_small\")\n",
        "df_keywords3= spark.read.option(\"header\",verdadero).parquet(input_bucket + 'ratings/')\n",
        "df_keywords3.createOrReplaceGlobalTempView(\"processed_ratings\")\n",
        "df_keywords4= spark.read.option(\"header\",verdadero).parquet(input_bucket +  'keywords/')\n",
        "df_keywords4.createOrReplaceGlobalTempView(\"processed_keywords\")\n",
        "df_keywords5= spark.read.option(\"header\",verdadero).parquet(input_bucket + 'credits/')\n",
        "df_keywords5.createOrReplaceGlobalTempView(\"processed_credits\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "auto",
        "id": "-NaLix2cp3dV"
      },
      "source": [
        "%spark.pyspark\n",
        "spark.catalog.listTables('default')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "auto",
        "id": "pi1W2ozPp3dW"
      },
      "source": [
        "%spark.pyspark\n",
        "\n",
        "def recomendacion(palbra_clave1, palabra_clave2, actor, actor2, actor3, cantidad_recomendaciones=3):\n",
        "    sql = \"\"\"\n",
        "        WITH movie_picks AS (\n",
        "           SELECT m.title,\n",
        "                date(m.release_date) AS release_date,\n",
        "                m.popularity, m.title,\n",
        "                count(r.movieid) AS ratings_count,\n",
        "                round(avg(r.rating), 3) AS avg_rating\n",
        "           FROM processed_movies_metadata AS m\n",
        "           LEFT JOIN processed_ratings AS r ON m.id = r.movieid\n",
        "           LEFT JOIN processed_keywords AS k ON m.id = k.id\n",
        "           LEFT JOIN processed_credits AS c ON m.id = c.id\n",
        "           WHERE (\n",
        "                    lower(k.keywords) LIKE '%{0}%'\n",
        "                    OR lower(k.keywords) LIKE '%{1}%'\n",
        "                )\n",
        "                AND (\n",
        "                    lower(c.cast) LIKE '%{2}%'\n",
        "                    OR lower(c.cast) LIKE '%{3}%'\n",
        "                    OR lower(c.cast) LIKE '%{4}%'\n",
        "                )\n",
        "            GROUP BY m.title,\n",
        "                m.release_date,\n",
        "                m.popularity\n",
        "            )\n",
        "            SELECT title,\n",
        "            ntile({5}) OVER (\n",
        "                ORDER BY avg_rating DESC\n",
        "            ) AS rank,\n",
        "            avg_rating,\n",
        "            ratings_count,\n",
        "            popularity,\n",
        "            release_date\n",
        "        FROM movie_picks\n",
        "        WHERE avg_rating > 0\n",
        "        ORDER BY rank,\n",
        "            avg_rating DESC,\n",
        "            ratings_count DESC\n",
        "       \"\"\".format(palbra_clave1, palabra_clave2, actor, actor2, actor3, cantidad_recomendaciones)\n",
        "       \n",
        "    return spark.sql(sql)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "auto",
        "id": "3E0qRRpzp3dX"
      },
      "source": [
        "%spark.pyspark\n",
        "\n",
        "palbra_clave1 = 'artificial intelligence'\n",
        "palabra_clave2 = 'robot'\n",
        "actor = 'arnold schwarzenegger'\n",
        "actor2 = 'jacki'\n",
        "actor3 =  'will smith'\n",
        "\n",
        "df_ratings = recomendaciones(palbra_clave1, palabra_clave2, actor, actor2, actor3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "auto",
        "id": "2c5iAzSFp3dX"
      },
      "source": [
        "%spark.pyspark\n",
        "df_ratings.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "auto",
        "id": "JGa776XXp3dY"
      },
      "source": [
        "%spark.pyspark\n",
        "sql = f\"\"\"\n",
        "    SELECT ntile(10) OVER (ORDER BY r.rating DESC) AS rank,\n",
        "           avg(r.rating) AS avg_rating,\n",
        "           count(r.movieid) AS ratings_count,\n",
        "           round(avg(cast(m.popularity AS DOUBLE)), 2) AS popularity\n",
        "       FROM processed_movies_metadata AS m\n",
        "           LEFT JOIN processed_ratings_small AS r ON m.id = r.movieid\n",
        "       WHERE r.rating IS NOT NULL\n",
        "           AND from_unixtime(r.timestamp) >= to_timestamp('2000-01-01')\n",
        "       GROUP BY r.rating\n",
        "   \"\"\"\n",
        "\n",
        "df_ratings = spark.sql(sql)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "autoscroll": "auto",
        "id": "2e1PZhEep3dY"
      },
      "source": [
        "%spark.pyspark\n",
        "df_ratings.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}